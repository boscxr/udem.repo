{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project: Computer Vision - Autonomous Robot\n",
    "\n",
    "- Elaborado por: Oscar Omar Martínez Lujano \n",
    "- Matrícula: 352228  \n",
    "- Carrera: ITR  \n",
    "- Fecha: 2019-05-15  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "La visión por computadora se está convirtiendo rápidamente en un factor clave en el alcance de la calidad dentro de los diferentes procesos de automatización. Los sistemas de visión por computadora estudian la interpretación de escenas a partir de proyecciones 2D mediante uno o mas sensores los cuales son conectados a un sistema informático.\\\\\n",
    "\n",
    "Su implementación en nuestra vida diaria, va en aumento y se ve reflejada por la implementación de sistemas inteligentes en los aparatos que utilizamos, en los juegos que jugamos, etc. La posibilidad de resolver problemas que antes eran imposibles, cada vez va en aumento ya que la tecnología de hardware se desarrolla cada día mas, y el software cada vez se realiza con mayor prestigio. Elevando los niveles de competitividad en el mercado nacional e internacional. Asimismo la visión por computadora permite inspeccionar el proceso de producción sin fatigas ni distracciones que son factores detonantes de errores humanos, facilitando la cuantificación de las variables de calidad traduciéndose en un mejoramiento continuo.\\\\\n",
    "\n",
    "Los sistemas de visión pueden ser implementados en lugares donde los seres humanos no podrían estar debido a las altas o bajas temperaturas, radiaciones, ruidos excesivos o ambientes peligrosos por diversos factores. En la actualidad los sistemas de visión por computadora han sido aplicados con éxito en muchas industrias como son: textil, frutas y alimentos, minerales, producción de circuitos integrados, autos, entre otros. En este caso, se realiza el desarrollo de un robot el cual controla su dirección para no salir de un carril, lo cual se aplica actualmente en automóviles autónomos junto con la integración de mas sensores. Se utilizan filtros y funciones matemáticas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "\n",
    "In this lab, you will learn about how to extract lines and circles from images using the Hough transform. Detection of features is an important part of image analysis and visual pattern recognition. Of particular interest is the identification of straight lines in images, as these appear in built indoor and outdoor environments as well as in manufactured parts. Hence, detecting lines in images play an important role in inspection, quality assessment, object detection, robot and vehicle navigation, etc. The images below illustrate three different environments where lines can be detected and can be used for scene understanding, autonomous vehicle navigation or electronic component inspection.\n",
    "\n",
    "<img src=\"Figs/udem.png\" width=\"800\" alt=\"Combined Image\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requerimientos del Laboratorio\n",
    "\n",
    "EL software, hardware y herramientas de programación requeridas en este lab son:\n",
    "  \n",
    "    - Raspberry Pi with WiFi connection capabilities\n",
    "    - Raspberry Cam\n",
    "    - Two batteries 5VDC 1A and 2A\n",
    "    - H Bridge\n",
    "    - Jupyter Notebook\n",
    "    - Cellphone with hotspot\n",
    "    - Python >= 3.5.2\n",
    "    - OpenCV 3.2\n",
    "    - Git\n",
    "    - GitHub account\n",
    "    - Markdown editor (recommended: ReText 5.3.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedimiento\n",
    "\n",
    "#### Line Hough Transform\n",
    "\n",
    "Thus far, the previous labs were focused on image processing algorithms such as colour space conversion, spatial filtering and edge detection. Today, this lab will be the first computer vision algorithm based on the Hough transform used for line detection in images. For a better understanding on how we can use the Hough transform to extract lines in images, we will go through a couple of concepts.\n",
    "\n",
    "The Hough transform is a parametric-model based technique. A parametric model can represent a class of instances where each is defined by a value of the parameters. A line and a circle are examples of parametrised models where the line equation: `y=mx+b` that is represented by its slope m, and its y-intercept b parameters. That means that different values of m and b would allow us to instantiate different lines.\n",
    "\n",
    "On the other hand, a circle can be parametrised by the equation:\n",
    "\n",
    "\n",
    "<img src=\"Figs/0.gif\" width=\"200\" alt=\"Combined Image\" />\n",
    "\n",
    "where (h, k) and r represent the circle centre location and its radius, respectively. Similarly, different values of these parameters would allow us to represent different circles.\n",
    "\n",
    "In order to detect lines in images, the Hough transform relies on image points belonging to edges. From our previous labs, we know how to extract edges either using the Sobel, Roberts, Scharr, and Prewitt operators.\n",
    "\n",
    "#### Voting\n",
    "\n",
    "Another concept to learn about is that of voting, the Hough transform relies on voting, which is a general approach where the features vote for all models that are compatible with it. It has been proved that voting techniques are robust to outliers. This is, even when noise and cluttered features participate on the voting process, voting given by good features are much more consistent. Furthermore, voting can deal with feature partially observed., providing still an appropriate set of parameters that explains the data points (edges) the best. When fitting a line to a set of points (edges), there are a few aspects to consider:\n",
    "\n",
    "    - How many points are those that belong to a given line?\n",
    "    - How many are there in the image?\n",
    "    - Which points belong to which lines?\n",
    "    \n",
    "To answer all of the above questions, we use the Hough transform. The idea behind this algorithm is as follows:\n",
    "\n",
    "    - 1. Each edge point votes for compatible lines\n",
    "    - 2. Look for lines that obtain the majority of votes\n",
    "    \n",
    "#### Hough Space\n",
    "\n",
    "The figure below illustrates how to Hough transform works. On the left coordinate frame we have a line representation in image space. This line is modelled by the parameters `m0-bo` that is mapped into a point in the Hough space on the right coordinate frame.\n",
    "\n",
    "<img src=\"Figs/1.svg\" width=\"600\" alt=\"Combined Image\" />\n",
    "\n",
    "we can now follow the duality principle and can assume that for a given potential edge point potential-edge-point we will obtain line in the Hough space following the next relationship between image and hough space:\n",
    "\n",
    "<img src=\"Figs/2hough.svg\" width=\"600\" alt=\"Combined Image\" />\n",
    "<img src=\"Figs/3hough.svg\" width=\"600\" alt=\"Combined Image\" />\n",
    "<img src=\"Figs/4hough.svg\" width=\"600\" alt=\"Combined Image\" />\n",
    "\n",
    "#### Line Detection Using OpenCV\n",
    "\n",
    "OpenCV provides us with two methods for line detection using the Hough transform. These are the standard Hough transform, `cv2.HoughLines()` and the Probabilistic Hough transform `cv2.HoughLinesP`. For those of you who may be interested in the maths and details on these two line detection techniques, please, refer to the following publications.\n",
    "\n",
    "- R. O.Duda and P.E. Hart, Use of the Hough Transformation to Detect Lines and Curves in Pictures, Graphics and Image Processing, Vol. 15 ,pp. 11-15, 1972.\n",
    "\n",
    "- N. Kiryati, Y. Eldar, and A. M. Bruckstein, A probabilistic Hough transform, Pattern Recognition, Vol. 24 (4), pp. 303-316, 1991.\n",
    "\n",
    "In this section, we will work on developing a vision-based driving assistance system (DAS) that will make use of the Hough transform to detect both the left and right lane lines on a road. The code below illustrate how this can be accomplished using OpenCV libraries. The first image represent the input colour image. Since the Hough transform works with single-channel images, it is necessary to convert the colour image into a greyscale format. This is shown in the second image below. This step is followed by the edge detection process show in the Canny image. These pixels are the ones used by the Probabilistic Hough transform for the line detection step that is shown in the last image.\n",
    "\n",
    "<img src=\"Figs/10.png\" width=\"600\" alt=\"Combined Image\" />\n",
    "<img src=\"Figs/11.png\" width=\"600\" alt=\"Combined Image\" />\n",
    "<img src=\"Figs/12.png\" width=\"600\" alt=\"Combined Image\" />\n",
    "<img src=\"Figs/13.png\" width=\"600\" alt=\"Combined Image\" />\n",
    "\n",
    "As can be seen from this last image, lines are not detected on the sky region since, but are mainly detected on the road and on the vegetation image sections. Since we are interested on extracting the road lane lines only, we can create a region of interest (roi) on that area that encloses the road lane on which the car is being driven, so that our vision system can output the following image:\n",
    "\n",
    "<img src=\"Figs/14.png\" width=\"600\" alt=\"Combined Image\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Libraries\n",
    "\n",
    "The following libraries are used in the code of this lab.\n",
    "\n",
    "- ```cv2```: Implementa una gran variedad de algorítmos de procesamiento de imágenes y visión computacional.\n",
    "- ```numpy:``` Crea y manipula listas, análisis numérico, etc.\n",
    "- `matplotlib.pyplot:` Produce publicaciones con figuras de calidad en una variedad de formatos de copia impresa y entornos interactivos en todas las plataformas.\n",
    "- ``time:`` This module provides various time-related functions. For related functionality, see also the datetime and calendar modules.\n",
    "- ``PiVideoStream:`` threading can be used to increase our FPS processing rate and reduce the affects of I/O latency on the Raspberry Pi.\n",
    "- ``RPi.GPIO:`` This package provides a class to control the GPIO on a Raspberry Pi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'picamera'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-015744aa152f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# imported required libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivideostream\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPiVideoStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/imutils/video/pivideostream.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import the necessary packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpicamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPiRGBArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpicamera\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPiCamera\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthreading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mThread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'picamera'"
     ]
    }
   ],
   "source": [
    "# imported required libraries\n",
    "from imutils.video.pivideostream import PiVideoStream\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from line import Line\n",
    "import RPi.GPIO as GPIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Robot Automation\n",
    "\n",
    "\n",
    "\n",
    "- `GPIO.output()\n",
    "    - WiringPi comes with a separate program to help manage the on-board GPIO interface as well as additional modules such as the PiFace and other devices like the Gertboard as well as generic GPIO expander type devices.\n",
    "- `cv2.line(img, pt1, pt2, color[, thickness[, lineType[, shift]]])`\n",
    "    - Draws a line segment connecting two points.\n",
    "    - img - Image\n",
    "    - pt1 – First point of the line segment.\n",
    "    - pt2 – Second point of the line segment.\n",
    "    - color – Line color.\n",
    "    - thickness – Line thickness.\n",
    "    - lineType – Type of the line:\n",
    "        - 8 (or omitted) - 8-connected line.\n",
    "        - 4 - 4-connected line.\n",
    "        - CV_AA - antialiased line.\n",
    "    - shift – Number of fractional bits in the point coordinates.\n",
    "    \n",
    "    \n",
    "- `matplotlib.pyplot.yticks(ticks=None, labels=None, **kwargs)[source]`\n",
    "\n",
    "    - **returns**: labels - a list of text objects\n",
    "\n",
    "\n",
    "Información obtenida de:\n",
    "- https://docs.opencv.org/2.4/modules/core/doc/drawing_functions.html#line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5045d76d3700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamedWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWINDOW_AUTOSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#set GPIO numbering mode and define output pins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mGPIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBOARD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mGPIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGPIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "cv2.namedWindow('main', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "#set GPIO numbering mode and define output pins\n",
    "GPIO.setmode(GPIO.BOARD)\n",
    "GPIO.setup(7,GPIO.OUT)\n",
    "GPIO.setup(11,GPIO.OUT)\n",
    "GPIO.setup(13,GPIO.OUT)\n",
    "GPIO.setup(15,GPIO.OUT)\n",
    "\n",
    "\n",
    "class ImageProcessor():\n",
    "\n",
    "    #Inicializamos las características de la imagen desplegada\n",
    "    def __init__(self,frameDimensions,frameRate):\n",
    "        self.frameDimensions=frameDimensions\n",
    "        self.frameRate = frameRate\n",
    "        self.w=self.frameDimensions[0]\n",
    "        self.h=self.frameDimensions[1]\n",
    "\n",
    "        #ROI DIMENSIONS %\n",
    "        self.roiY = (0.45,0.8)\n",
    "        self.roiX = (0.90, 1)\n",
    "\n",
    "\n",
    "        self.left=Line(self.frameDimensions, (255,0,60))\n",
    "        self.right=Line(self.frameDimensions,(70,122,180))\n",
    "\n",
    "\n",
    "    #Se realiza el filtro Blur\n",
    "    def makeBlur(self, frame, iterations, kernelSize):\n",
    "        blured = frame.copy()\n",
    "        while iterations > 0:\n",
    "            blured = cv2.GaussianBlur(blured,(kernelSize,kernelSize),sigmaX=0,sigmaY=0)\n",
    "            iterations-=1\n",
    "        return blured\n",
    "\n",
    "    #Se aplica la región de interes en porcentaje\n",
    "    def makeRoi(self, frame):\n",
    "        y0Px = self.h * self.roiY[0]\n",
    "        y1Px = self.h * self.roiY[1]\n",
    "        x0Px = (1 - self.roiX[0]) * self.w / 2\n",
    "        x1Px = (1 - self.roiX[1]) * self.w / 2\n",
    "        vertices = np.array([[\n",
    "            (x0Px, y0Px),\n",
    "            (x1Px, y1Px),\n",
    "            (self.w - x1Px, y1Px),\n",
    "            (self.w - x0Px, y0Px)\n",
    "        ]], dtype=np.int32)\n",
    "        mask = np.zeros_like(frame)\n",
    "        cv2.fillPoly(mask, vertices, 255)\n",
    "        return cv2.bitwise_and(frame, mask)\n",
    "\n",
    "\n",
    "    #Se encuentran las lineas con sus respectivos slopes y en el área de interes.\n",
    "    def findLanes(self, frame, lines, drawAll=False, minAngle=10):\n",
    "        self.left.clear()\n",
    "        self.right.clear()\n",
    "        if type(lines) == type(np.array([])):\n",
    "            for line in lines:\n",
    "                for x1, y1, x2, y2 in line:\n",
    "                    angle = np.degrees(np.arctan2(y2 - y1, x2 - x1))\n",
    "                    if np.abs(angle) > minAngle:\n",
    "                        if angle > 0:\n",
    "                            self.right.add(x1, y1, x2, y2)\n",
    "                            if drawAll:\n",
    "                                cv2.line(frame, (x1, y1), (x2, y2), self.right.color)\n",
    "                        else:\n",
    "                            self.left.add(x1, y1, x2, y2)\n",
    "                            if drawAll:\n",
    "                                cv2.line(frame, (x1, y1), (x2, y2), self.left.color)\n",
    "        self.left.fit()\n",
    "        self.right.fit()\n",
    "        return frame\n",
    "\n",
    "    #Se realiza el poligono que se despliega en la pantalla\n",
    "    def drawPoly(self, frame, poly, color):\n",
    "        y0Px = int(self.h * self.roiY[0])\n",
    "        y1Px = int(self.h * self.roiY[1])\n",
    "        if poly:\n",
    "            x0Px = int(poly(y0Px))\n",
    "            x1Px = int(poly(y1Px))\n",
    "            cv2.line(frame,(x0Px,y0Px),(x1Px,y1Px),color, 3)\n",
    "\n",
    "    #Se procesan los frames que recibe aplicando diferentes filtros\n",
    "    def process(self,frame):\n",
    "        gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        grayColor=cv2.cvtColor(gray,cv2.COLOR_GRAY2BGR)\n",
    "        blured=self.makeBlur(gray, iterations=2,kernelSize=7)\n",
    "        canny=cv2.Canny(blured,threshold1=10,threshold2=50)\n",
    "        roi=self.makeRoi(canny)\n",
    "        rho=1\n",
    "        #line_image= np.copy(img)*0  #create a blank to draw lines on\n",
    "        hough_lines= cv2.HoughLinesP(roi,rho,theta= np.pi/180,threshold=20, \n",
    "        lines=np.array([]), minLineLength=5, maxLineGap=60)\n",
    "\n",
    "        lanes = self.findLanes(grayColor,hough_lines,drawAll=True,minAngle=10)\n",
    "        self.drawPoly(lanes, self.left.poly, self.left.color)\n",
    "        self.drawPoly(lanes, self.right.poly, self.right.color)\n",
    "\n",
    "        return lanes\n",
    "\n",
    "#Se definen parámetros\n",
    "processor=ImageProcessor((480,320),20)\n",
    "camera=PiVideoStream(resolution=processor.frameDimensions,framerate=processor.frameRate).start()\n",
    "time.sleep(2)\n",
    "\n",
    "#esta es la k para encontrar la pendiente de las lineas cuando el carro se encuentra en el centro del carril\n",
    "#mk+b\n",
    "avgleft=np.poly1d(np.array([-2.92347668,580.29388073]))\n",
    "avgright=np.poly1d(np.array([3.33334898,-189.29432108]))\n",
    "\n",
    "def main():\n",
    "\n",
    "    while True:\n",
    "        frame = camera.read()\n",
    "        out=processor.process(frame)\n",
    "\n",
    "        processor.drawPoly(out,avgleft,(255,255,255))\n",
    "        processor.drawPoly(out,avgright,(255,255,255))\n",
    "\n",
    "        #control\n",
    "        # si hay lineas, encuentra la distancia que hay entre los coeficientes de la linea generada\n",
    "        # entre las que esta encontrando en ese momento\n",
    "        if processor.left.poly and processor.right.poly:\n",
    "            distancel=avgleft(processor.roiY[0]*processor.h)-processor.left.poly(processor.roiY[0]*processor.h)\n",
    "            distancer=avgright(processor.roiY[0]*processor.h)-processor.right.poly(processor.roiY[0]*processor.h)\n",
    "            print(distancer)\n",
    "            \n",
    "            #if the distance is greater than 35 then the car will move left\n",
    "            if distancer > 35:\n",
    "                GPIO.output(7,False)\n",
    "                GPIO.output(11,True)\n",
    "                GPIO.output(13,False)\n",
    "                GPIO.output(15,True)\n",
    "            #if the distance is less than 35 then the car will move right\n",
    "            elif distancer<-35:\n",
    "                GPIO.output(7,True)\n",
    "                GPIO.output(11,False)\n",
    "                GPIO.output(13,True)\n",
    "                GPIO.output(15,False)\n",
    "            #if the distance is between -35 and  35 then the car will move forward\n",
    "            else:\n",
    "                GPIO.output(7,True)\n",
    "                GPIO.output(11,False)\n",
    "                GPIO.output(13,False)\n",
    "                GPIO.output(15,True)\n",
    "\n",
    "        else:\n",
    "            #if the camera doesnt detect lines, the car stops\n",
    "            GPIO.output(7,False)\n",
    "            GPIO.output(11,False)\n",
    "            GPIO.output(13,False)\n",
    "            GPIO.output(15,False)\n",
    "\n",
    "        #despliega la imagen\n",
    "        cv2.imshow('main',out)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        #imprime en la consola los coefientes de las lineas detectadas\n",
    "        elif key == ord('k'):\n",
    "            print(processor.left.poly.coeffs)\n",
    "            print(processor.right.poly.coeffs)\n",
    "\n",
    "        #derecha\n",
    "        elif key == ord('d'):\n",
    "            GPIO.output(7,True)\n",
    "            GPIO.output(11,False)\n",
    "            GPIO.output(13,True)\n",
    "            GPIO.output(15,False)\n",
    "\n",
    "        #adelante\n",
    "        elif key == ord('w'):\n",
    "            GPIO.output(7,True)\n",
    "            GPIO.output(11,False)\n",
    "            GPIO.output(13,False)\n",
    "            GPIO.output(15,True)\n",
    "\n",
    "        #izquierda\n",
    "        elif key == ord('a'):\n",
    "            GPIO.output(7,False)\n",
    "            GPIO.output(11,True)\n",
    "            GPIO.output(13,False)\n",
    "            GPIO.output(15,True)\n",
    "        #atras\n",
    "        elif key == ord('s'):\n",
    "            GPIO.output(7,False)\n",
    "            GPIO.output(11,True)\n",
    "            GPIO.output(13,True)\n",
    "            GPIO.output(15,False)\n",
    "        #stop con el enter\n",
    "        elif key == 10:\n",
    "            GPIO.output(7,False)\n",
    "            GPIO.output(11,False)\n",
    "            GPIO.output(13,False)\n",
    "            GPIO.output(15,False)\n",
    "\n",
    "    camera.stop()\n",
    "    GPIO.cleanup()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class generate the lines\n",
    "#!/usr/bin/python3\n",
    "import numpy as np\n",
    "\n",
    "class Line():\n",
    "\n",
    "    def __init__(self, frameDimensions,color):\n",
    "        self.frameDimensions = frameDimensions\n",
    "        self.color = color\n",
    "        self.w = frameDimensions[0]\n",
    "        self.h = frameDimensions[1]\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.poly = None\n",
    "\n",
    "    def add(self, x0, y0, x1, y1):\n",
    "        self.x.extend([x0, x1])\n",
    "        self.y.extend([y0, y1])\n",
    "\n",
    "    def clear(self):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "\n",
    "    def fit(self):\n",
    "        if len(self.x) > 0 and len(self.y) > 0:\n",
    "            self.poly = np.poly1d(np.polyfit(self.y, self.x, deg=1))\n",
    "        else:\n",
    "            self.poly = None    \n",
    "        return self.poly\n",
    "\n",
    "    def eval(self, y0, y1):\n",
    "        y0Px = int(self.h * y0)\n",
    "        y1Px = int(self.h * y1)\n",
    "        if self.poly:\n",
    "            return [\n",
    "                int(self.poly(y0Px)),\n",
    "                y0Px,\n",
    "                int(self.poly(y1Px)),\n",
    "                y1Px\n",
    "            ]\n",
    "        else:\n",
    "            return [0, y0Px, 0, y1Px]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Videos del robot en la pista\n",
    "\n",
    "https://drive.google.com/drive/folders/1Ez1vsHjAnH3bYhZU4eGI-tk6mupwj9y1?usp=sharing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "Al terminar este proyecto, y desarrollar un sistema inteligente capaz de relacionar el entorno y utilizar la información recabada de la cámara para su beneficio. Utilizamos diferentes disciplinas y una de ellas fue el gran uso de numpy de opencv. Realizar este proyecto fue bastante retador da pesar de que todo el semestre los temas que vimos, fueron relacionados al proyecto. Y me dejó una gran experiencia el trabajar junto con mis compañeñeros. Un sistema autónomo es muy divertido de realizar y sin duda fue el proyecto mas bonito que realice en toda mi carrera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Yo declaro, que he realizado este Proyecto Final con integridad académica_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
